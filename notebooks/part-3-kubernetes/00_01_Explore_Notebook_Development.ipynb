{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Python packages in the current Jupyter Kernel\n",
    "\n",
    "If you're in the jupyter notebook and you want to install a package with `pip`, you might be tempted to use the `!` notation to run pip directly as a shell command from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (1.18.1)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 20.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop your model and run inside notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "32768/29515 [=================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26427392/26421880 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "8192/5148 [===============================================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4423680/4422102 [==============================] - 0s 0us/step\n",
      "\n",
      "train_images.shape: (60000, 28, 28, 1), of float64\n",
      "test_images.shape: (10000, 28, 28, 1), of float64\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Conv1 (Conv2D)               (None, 13, 13, 8)         80        \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1352)              0         \n",
      "_________________________________________________________________\n",
      "Softmax (Dense)              (None, 10)                13530     \n",
      "=================================================================\n",
      "Total params: 13,610\n",
      "Trainable params: 13,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.5278 - acc: 0.8175\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 4s 69us/sample - loss: 0.3831 - acc: 0.8666\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 4s 68us/sample - loss: 0.3511 - acc: 0.8773\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 4s 71us/sample - loss: 0.3334 - acc: 0.8828\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 4s 74us/sample - loss: 0.3209 - acc: 0.8855\n",
      "10000/10000 [==============================] - 1s 50us/sample - loss: 0.3601 - acc: 0.8706\n",
      "\n",
      "Test accuracy: 0.8705999851226807\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import subprocess\n",
    "import argparse\n",
    "\n",
    "# Reduce spam logs from s3 client\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "\n",
    "def preprocessing():\n",
    "  fashion_mnist = keras.datasets.fashion_mnist\n",
    "  (train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
    "\n",
    "  # scale the values to 0.0 to 1.0\n",
    "  train_images = train_images / 255.0\n",
    "  test_images = test_images / 255.0\n",
    "\n",
    "  # reshape for feeding into the model\n",
    "  train_images = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "  test_images = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "\n",
    "  class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "                'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "  print('\\ntrain_images.shape: {}, of {}'.format(train_images.shape, train_images.dtype))\n",
    "  print('test_images.shape: {}, of {}'.format(test_images.shape, test_images.dtype))\n",
    "\n",
    "  return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "def train(train_images, train_labels, epochs, model_summary_path):\n",
    "  if model_summary_path:\n",
    "    logdir=model_summary_path # + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "  model = keras.Sequential([\n",
    "    keras.layers.Conv2D(input_shape=(28,28,1), filters=8, kernel_size=3,\n",
    "                        strides=2, activation='relu', name='Conv1'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(10, activation=tf.nn.softmax, name='Softmax')\n",
    "  ])\n",
    "  model.summary()\n",
    "\n",
    "  model.compile(optimizer=tf.train.AdamOptimizer(),\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy']\n",
    "                )\n",
    "  if model_summary_path:\n",
    "    model.fit(train_images, train_labels, epochs=epochs, callbacks=[tensorboard_callback])\n",
    "  else:\n",
    "    model.fit(train_images, train_labels, epochs=epochs)\n",
    "\n",
    "  return model\n",
    "\n",
    "def eval(model, test_images, test_labels):\n",
    "  test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "  print('\\nTest accuracy: {}'.format(test_acc))\n",
    "\n",
    "def export_model(model, model_export_path):\n",
    "  version = 1\n",
    "  export_path = os.path.join(model_export_path, str(version))\n",
    "\n",
    "  tf.saved_model.simple_save(\n",
    "    keras.backend.get_session(),\n",
    "    export_path,\n",
    "    inputs={'input_image': model.input},\n",
    "    outputs={t.name:t for t in model.outputs})\n",
    "\n",
    "  print('\\nSaved model: {}'.format(export_path))\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "  parser = argparse.ArgumentParser(description='Fashion MNIST Tensorflow Example')\n",
    "  parser.add_argument('--model_export_path', type=str, help='Model export path')\n",
    "  parser.add_argument('--model_summary_path', type=str,  help='Model summry files for Tensorboard visualization')\n",
    "  parser.add_argument('--epochs', type=int, default=5, help='Training epochs')\n",
    "  args = parser.parse_args(args=[])\n",
    "\n",
    "  train_images, train_labels, test_images, test_labels = preprocessing()\n",
    "  model = train(train_images, train_labels, args.epochs, args.model_summary_path)\n",
    "  eval(model, test_images, test_labels)\n",
    "\n",
    "  if args.model_export_path:\n",
    "    export_model(model, args.model_export_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing your notebooks\n",
    "\n",
    "When we talk of sharing their notebooks, there are generally two paradigms they may be considering. Most often, individuals share the end-result of their work, which means sharing non-interactive, pre-rendered versions of their notebooks; however, it is also possible to collaborate on notebooks with the aid version control systems such as Git.\n",
    "\n",
    "### Before your share\n",
    "A shared notebook will appear exactly in the state it was in when you export or save it, including the output of any code cells. Therefore, to ensure that your notebook is share-ready, so to speak, there are a few steps you should take before sharing:\n",
    "\n",
    "- Click \"Cell > All Output > Clear\"\n",
    "- Click \"Kernel > Restart & Run All\"\n",
    "- Wait for your code cells to finish executing and check they did so as expected\n",
    "\n",
    "This will ensure your notebooks donâ€™t contain intermediary output, have a stale state, and executed in order at the time of sharing.\n",
    "\n",
    "- Click \"Cell > All Output > Clear\" (One more time to clearn up results)\n",
    "- Click \"File > Download as > Notebook\" and export your notebook\n",
    "\n",
    "\n",
    "### Workspace Volume\n",
    "By default, when you delete your notebook, workspace volume will not be deleted. You can persist your model development work here. Then, You and your teammate can reuse these existing volumes. \n",
    "\n",
    "![workspace_volume](./images/workspace_volume.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
