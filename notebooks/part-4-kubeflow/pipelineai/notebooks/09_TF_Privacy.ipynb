{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Differential Privacy\n",
    "More information here:  https://github.com/tensorflow/privacy\n",
    "\n",
    "## IMPORTANT:  You Should STOP All Kernels and Terminal Sessions\n",
    "The GPU may be wedged at this point.  We stop all kernels and terminal sessions to release the GPU resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Privacy for Hyper-Parameters\n",
    "\n",
    "Apply the RDP accountant to estimate privacy budget of an iterated Sampled Gaussian Mechanism. \n",
    "\n",
    "The output states that DP-SGD with these parameters satisfies (2.92, 1e-5)-DP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP-SGD with sampling rate = 5.33% and noise_multiplier = 1.12 iterated over 19 steps satisfies differential privacy with eps = 2.49 and delta = 1e-05.\n",
      "The optimal RDP order is 7.0.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "\n",
    "from absl import app\n",
    "from absl import flags\n",
    "\n",
    "from privacy.analysis.rdp_accountant import compute_rdp\n",
    "from privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "\n",
    "N = 600 # Total number of examples\n",
    "batch_size = 32 # batch size\n",
    "noise_multiplier = 1.12 # Noise multiplier for DP-SGD\n",
    "epochs = 1 # Number of epochs (may be fractional)\n",
    "delta = 1e-5 # Target delta\n",
    "\n",
    "def apply_dp_sgd_analysis(q, sigma, steps, orders, delta):\n",
    "  \"\"\"Compute and print results of DP-SGD analysis.\"\"\"\n",
    "\n",
    "  rdp = compute_rdp(q, sigma, steps, orders)\n",
    "\n",
    "  eps, _, opt_order = get_privacy_spent(orders, rdp, target_delta=delta)\n",
    "\n",
    "  print('DP-SGD with sampling rate = {:.3g}% and noise_multiplier = {} iterated'\n",
    "        ' over {} steps satisfies'.format(100 * q, sigma, steps), end=' ')\n",
    "  print('differential privacy with eps = {:.3g} and delta = {}.'.format(\n",
    "      eps, delta))\n",
    "  print('The optimal RDP order is {}.'.format(opt_order))\n",
    "\n",
    "  if opt_order == max(orders) or opt_order == min(orders):\n",
    "    print('The privacy estimate is likely to be improved by expanding '\n",
    "          'the set of orders.')\n",
    "\n",
    "q = batch_size / N  # q - the sampling ratio.\n",
    "\n",
    "if q > 1:\n",
    "  raise app.UsageError('N must be larger than the batch size.')\n",
    "\n",
    "orders = ([1.25, 1.5, 1.75, 2., 2.25, 2.5, 3., 3.5, 4., 4.5] + list(range(5, 64)) +\n",
    "            [128, 256, 512])\n",
    "\n",
    "steps = int(math.ceil(epochs * N / batch_size))\n",
    "\n",
    "apply_dp_sgd_analysis(q, noise_multiplier, steps, orders, delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train with Differential Privacy SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Training a CNN on MNIST with differentially private SGD optimizer.\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from privacy.analysis.rdp_accountant import compute_rdp\n",
    "from privacy.analysis.rdp_accountant import get_privacy_spent\n",
    "from privacy.optimizers import dp_optimizer\n",
    "\n",
    "dpsgd = True # If True, train with DP-SGD. If False, train with vanilla SGD.\n",
    "learning_rate = 0.08 # Learning rate for training\n",
    "noise_multiplier = 1.12 # Ratio of the standard deviation to the clipping norm\n",
    "l2_norm_clip = 1.0 # Clipping norm\n",
    "batch_size = 32 # Batch size\n",
    "epochs = 1 # Number of epochs\n",
    "microbatches = 32 # Number of microbatches (must evenly divide batch_size\n",
    "model_dir = None # Model directory\n",
    "export_dir = './pipeline_tfserving/0' # Export dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/tmp/tmp6thgs9qm\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/tmp/tmp6thgs9qm', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa2322b9208>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_queue_runner.py:62: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow_estimator/python/estimator/inputs/queues/feeding_functions.py:500: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/training/monitored_session.py:809: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /var/tmp/tmp6thgs9qm/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.5261045, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 18 into /var/tmp/tmp6thgs9qm/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.803492.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/metrics_impl.py:455: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-01-23T03:46:36Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /var/tmp/tmp6thgs9qm/model.ckpt-18\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-01-23-03:46:37\n",
      "INFO:tensorflow:Saving dict for global step 18: accuracy = 0.1682, global_step = 18, loss = 2.4149764\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 18: /var/tmp/tmp6thgs9qm/model.ckpt-18\n",
      "Test accuracy after 1 epochs is: 0.168\n",
      "For delta=1e-5, the current epsilon is: 2.45\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "  \"\"\"Model function for a CNN.\"\"\"\n",
    "\n",
    "  # Define CNN architecture using tf.keras.layers.\n",
    "  input_layer = tf.reshape(features['x'], [-1, 28, 28, 1])\n",
    "  y = tf.keras.layers.Conv2D(16, 8,\n",
    "                             strides=2,\n",
    "                             padding='same',\n",
    "                             kernel_initializer='he_normal').apply(input_layer)\n",
    "  y = tf.keras.layers.MaxPool2D(2, 1).apply(y)\n",
    "  y = tf.keras.layers.Conv2D(32, 4,\n",
    "                             strides=2,\n",
    "                             padding='valid',\n",
    "                             kernel_initializer='he_normal').apply(y)\n",
    "  y = tf.keras.layers.MaxPool2D(2, 1).apply(y)\n",
    "  y = tf.keras.layers.Flatten().apply(y)\n",
    "  y = tf.keras.layers.Dense(32, kernel_initializer='he_normal').apply(y)\n",
    "  logits = tf.keras.layers.Dense(10, kernel_initializer='he_normal').apply(y)\n",
    "\n",
    "  # Calculate loss as a vector (to support microbatches in DP-SGD).\n",
    "  vector_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "      labels=labels, logits=logits)\n",
    "  # Define mean of loss across minibatch (for reporting through tf.Estimator).\n",
    "  scalar_loss = tf.reduce_mean(vector_loss)\n",
    "\n",
    "  # Configure the training op (for TRAIN mode).\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "    if dpsgd:\n",
    "      # Use DP version of GradientDescentOptimizer. For illustration purposes,\n",
    "      # we do that here by calling make_optimizer_class() explicitly, though DP\n",
    "      # versions of standard optimizers are available in dp_optimizer.\n",
    "      dp_optimizer_class = dp_optimizer.make_optimizer_class(\n",
    "          tf.train.GradientDescentOptimizer)\n",
    "      optimizer = dp_optimizer_class(\n",
    "          learning_rate=learning_rate,\n",
    "          noise_multiplier=noise_multiplier,\n",
    "          l2_norm_clip=l2_norm_clip,\n",
    "          num_microbatches=microbatches)\n",
    "      opt_loss = vector_loss\n",
    "    else:\n",
    "      optimizer = tf.train.GradientDescentOptimizer(\n",
    "          learning_rate=learning_rate)\n",
    "      opt_loss = scalar_loss\n",
    "    global_step = tf.train.get_global_step()\n",
    "    train_op = optimizer.minimize(loss=opt_loss, global_step=global_step)\n",
    "    # In the following, we pass the mean of the loss (scalar_loss) rather than\n",
    "    # the vector_loss because tf.estimator requires a scalar loss. This is only\n",
    "    # used for evaluation and debugging by tf.estimator. The actual loss being\n",
    "    # minimized is opt_loss defined above and passed to optimizer.minimize().\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      loss=scalar_loss,\n",
    "                                      train_op=train_op)\n",
    "\n",
    "  # Add evaluation metrics (for EVAL mode).\n",
    "  elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "    eval_metric_ops = {\n",
    "        'accuracy':\n",
    "            tf.metrics.accuracy(\n",
    "                labels=labels,\n",
    "                predictions=tf.argmax(input=logits, axis=1))\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      loss=scalar_loss,\n",
    "                                      eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "  \"\"\"Loads MNIST and preprocesses to combine training and validation data.\"\"\"\n",
    "  train, test = tf.keras.datasets.mnist.load_data()\n",
    "  train_data, train_labels = train\n",
    "  test_data, test_labels = test\n",
    "\n",
    "  train_data = np.array(train_data, dtype=np.float32) / 255\n",
    "  test_data = np.array(test_data, dtype=np.float32) / 255\n",
    "\n",
    "  train_labels = np.array(train_labels, dtype=np.int32)\n",
    "  test_labels = np.array(test_labels, dtype=np.int32)\n",
    "\n",
    "  assert train_data.min() == 0.\n",
    "  assert train_data.max() == 1.\n",
    "  assert test_data.min() == 0.\n",
    "  assert test_data.max() == 1.\n",
    "  assert len(train_labels.shape) == 1\n",
    "  assert len(test_labels.shape) == 1\n",
    "\n",
    "  return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "if batch_size % microbatches != 0:\n",
    "  raise ValueError('Number of microbatches should divide evenly batch_size')\n",
    "\n",
    "# Load training and test data.\n",
    "train_data, train_labels, test_data, test_labels = load_mnist()\n",
    "\n",
    "# Instantiate the tf.Estimator.\n",
    "mnist_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                                            model_dir=model_dir)\n",
    "\n",
    "# Create tf.Estimator input functions for the training and test data.\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={'x': train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=batch_size,\n",
    "      num_epochs=epochs,\n",
    "      shuffle=True)\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={'x': test_data},\n",
    "      y=test_labels,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "\n",
    "# Define a function that computes privacy budget expended so far.\n",
    "def compute_epsilon(steps):\n",
    "  \"\"\"Computes epsilon value for given hyperparameters.\"\"\"\n",
    "  if noise_multiplier == 0.0:\n",
    "    return float('inf')\n",
    "  orders = [1 + x / 10. for x in range(1, 100)] + list(range(12, 64))\n",
    "  sampling_probability = batch_size / N\n",
    "  rdp = compute_rdp(q=sampling_probability,\n",
    "                      noise_multiplier=noise_multiplier,\n",
    "                      steps=steps,\n",
    "                      orders=orders)\n",
    "    # Delta is set to 1e-5 because MNIST has N training points.\n",
    "  return get_privacy_spent(orders, rdp, target_delta=1e-5)[0]\n",
    "\n",
    "# Training loop.\n",
    "steps_per_epoch = N // batch_size\n",
    "for epoch in range(1, epochs + 1):\n",
    "  # Train the model for one epoch.\n",
    "  mnist_classifier.train(input_fn=train_input_fn, steps=steps_per_epoch)\n",
    "\n",
    "  # Evaluate the model and print results\n",
    "  eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "  test_accuracy = eval_results['accuracy']\n",
    "  print('Test accuracy after %d epochs is: %.3f' % (epoch, test_accuracy))\n",
    "\n",
    "  # Compute the privacy budget expended so far.\n",
    "  if dpsgd:\n",
    "    eps = compute_epsilon(epoch * steps_per_epoch)\n",
    "    print('For delta=1e-5, the current epsilon is: %.2f' % eps)\n",
    "  else:\n",
    "    print('Trained with vanilla non-private SGD optimizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
